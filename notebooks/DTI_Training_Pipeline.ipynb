{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Setup and Imports\n",
    "\n",
    "This cell imports all necessary libraries. We import standard data science tools (`pandas`, `sklearn`), PyTorch, and crucially, PyTorch Geometric (`torch_geometric`) for handling graph data. We also import `os` and `sys` to manage our project paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Dataset, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_recall_curve, auc\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Path Configuration & Custom Module Imports\n",
    "\n",
    "This is a critical step for running notebooks inside a project. We add the `src` directory (which is one level up, `../`) to the system path. This allows us to import our own custom Python modules (`FeatureEngineer`, `DTIPredictor`, `load_config`) just like in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PATH = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "\n",
    "try:\n",
    "    from preprocessing.feature_engineer import FeatureEngineer\n",
    "    from models.dti_model import DTIPredictor\n",
    "    from utils.config_loader import load_config\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing custom modules: {e}\")\n",
    "    print(f\"Please ensure __init__.py files exist in src subdirectories and all requirements are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: Load Configuration and Set Hyperparameters\n",
    "\n",
    "Instead of hard-coding parameters, we load the *exact same* `config.yaml` file the Streamlit app uses. This ensures consistency between training and deployment. We define our training-specific hyperparameters (epochs, batch size, etc.) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "CONFIG_PATH = '../config/config.yaml'\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATION_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Loaded model hyperparameters: {config['model']['hyperparameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Load Training Data\n",
    "\n",
    "We load the `relationships.tsv` file. This file should contain the ground-truth data, including SMILES strings for drugs, amino acid sequences for proteins, and the binary label (1 for interaction, 0 for no interaction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/relationships.tsv'\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, sep='\\t')\n",
    "    # We assume columns are named 'drug_smiles', 'protein_sequence', 'label'\n",
    "    # Adjust column names if your .tsv file is different\n",
    "    print(f\"Loaded data with {len(df)} samples.\")\n",
    "    print(df.head())\n",
    "    print(\"\\nData Info:\")\n",
    "    df.info()\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(df['label'].value_counts(normalize=True))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {DATA_PATH}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing expected column {e}. Please check your .tsv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Define Custom PyTorch Geometric Dataset\n",
    "\n",
    "This is the core of our data pipeline. We create a custom `Dataset` class that integrates our `FeatureEngineer`.\n",
    "\n",
    "1.  `__init__`: Stores the DataFrame and an instance of our `FeatureEngineer`.\n",
    "2.  `len`: Returns the total number of samples.\n",
    "3.  `get`: This is the magic. For a given index `idx`:\n",
    "    * It fetches the SMILES string and protein sequence.\n",
    "    * It uses `self.feature_engineer.featurize()` to convert them into a PyG `Data` object (a graph).\n",
    "    * It attaches the `label` to the graph object as `data.y`.\n",
    "    * It includes robust error handling, as featurization for complex molecules can sometimes fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, df, feature_engineer):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.feature_engineer = feature_engineer\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        try:\n",
    "            # Adjust column names if different\n",
    "            smiles = row['drug_smiles']\n",
    "            sequence = row['protein_sequence']\n",
    "            label = row['label']\n",
    "\n",
    "            # Use the feature engineer to create the graph data object\n",
    "            data = self.feature_engineer.featurize(smiles, sequence)\n",
    "            \n",
    "            # Attach the label\n",
    "            data.y = torch.tensor([label], dtype=torch.float)\n",
    "            return data\n",
    "        \n",
    "        except Exception as e:\n",
    "            #print(f\"Warning: Skipping index {idx}. Failed to featurize: {e}\")\n",
    "            return None # Will be filtered by our custom collate function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: Instantiate Dataset and Create DataLoaders\n",
    "\n",
    "1.  **Instantiate `FeatureEngineer`**: We use the settings from our `config.yaml`.\n",
    "2.  **Instantiate `DTIDataset`**: We pass the DataFrame and the feature engineer to it.\n",
    "3.  **Split Data**: We split the full dataset into training and validation sets.\n",
    "4.  **Define `collate_fn`**: This function is crucial. It gathers a list of `Data` objects into a `Batch` and filters out any `None` values that resulted from featurization errors.\n",
    "5.  **Create `DataLoaders`**: We create PyG `DataLoader`s for both train and validation sets, using our `collate_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate FeatureEngineer\n",
    "feature_engineer = FeatureEngineer(config['model']['featurization'])\n",
    "\n",
    "# 2. Instantiate Dataset\n",
    "print(\"Initializing dataset... This may take a moment as it checks data.\")\n",
    "dataset = DTIDataset(df, feature_engineer)\n",
    "print(\"Dataset initialized.\")\n",
    "\n",
    "# 3. Split Data\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "val_size = int(len(dataset) * VALIDATION_SPLIT)\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# 4. Define custom collate function to filter None values\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "# 5. Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7: Define Model, Loss, and Optimizer\n",
    "\n",
    "1.  **Model**: We instantiate our `DTIPredictor` using the hyperparameters from `config.yaml` and move it to the `DEVICE`.\n",
    "2.  **Loss Function**: We use `BCEWithLogitsLoss`, which is standard for binary classification. It's numerically stable and expects raw logits from the model.\n",
    "3.  **Optimizer**: We use `Adam`, a robust and popular optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate Model\n",
    "model_params = config['model']['hyperparameters']\n",
    "model = DTIPredictor(**model_params).to(DEVICE)\n",
    "\n",
    "# 2. Define Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss() # For binary classification\n",
    "\n",
    "# 3. Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "print(f\"Model loaded on {DEVICE}.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8: Training and Validation Loop\n",
    "\n",
    "This is the main training logic. For each epoch, we:\n",
    "\n",
    "1.  **Train (`model.train()`):**\n",
    "    * Iterate through the `train_loader`.\n",
    "    * Move the batch of graphs to the `DEVICE`.\n",
    "    * Perform the forward pass: `output = model(batch)`.\n",
    "    * Calculate the loss.\n",
    "    * Perform the backward pass (`loss.backward()`) and update weights (`optimizer.step()`).\n",
    "\n",
    "2.  **Validate (`model.eval()`):**\n",
    "    * Iterate through the `val_loader` with `torch.no_grad()`.\n",
    "    * Collect all predictions (`all_preds`) and true labels (`all_labels`).\n",
    "    * Calculate metrics: Loss, Accuracy, ROC-AUC, and F1-Score.\n",
    "\n",
    "3.  **Save Best Model**: We track the best validation AUC and save the model weights (`.pt` file) only when it improves. This prevents overfitting and ensures we save the most performant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR = '../models'\n",
    "MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_DIR, 'dti_model.pt')\n",
    "\n",
    "# Ensure the model directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "best_val_auc = 0.0\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    \n",
    "    for batch in train_loop:\n",
    "        if batch is None: # Skip bad batches\n",
    "            continue\n",
    "        \n",
    "        batch = batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            if batch is None: # Skip bad batches\n",
    "                continue\n",
    "            \n",
    "            batch = batch.to(DEVICE)\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch.y)\n",
    "            val_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            preds = torch.sigmoid(output)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(batch.y.cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if not all_labels or not all_preds:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Validation data empty. Skipping metrics.\")\n",
    "        continue\n",
    "        \n",
    "    all_preds = torch.cat(all_preds).numpy().flatten()\n",
    "    all_labels = torch.cat(all_labels).numpy().flatten()\n",
    "    \n",
    "    val_auc = roc_auc_score(all_labels, all_preds)\n",
    "    val_preds_binary = (all_preds > 0.5).astype(int)\n",
    "    val_accuracy = accuracy_score(all_labels, val_preds_binary)\n",
    "    val_f1 = f1_score(all_labels, val_preds_binary)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val AUC: {val_auc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"  -> New best model saved to {MODEL_SAVE_PATH} (Val AUC: {best_val_auc:.4f})\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best validation AUC achieved: {best_val_auc:.4f}\")\n",
    "print(f\"Final model saved at: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 9: Final Check\n",
    "\n",
    "This cell just verifies that the model file was created in the correct location. Once this notebook is run, your Streamlit application will have the `dti_model.pt` file it needs to load the `CoreProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"SUCCESS: Model file found at {MODEL_SAVE_PATH}\")\n",
    "    print(f\"File size: {os.path.getsize(MODEL_SAVE_PATH) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"ERROR: Model file was NOT created at {MODEL_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
