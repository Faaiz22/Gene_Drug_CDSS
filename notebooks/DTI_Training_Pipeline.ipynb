{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTI Model Training Pipeline\n",
    "\n",
    "This notebook imports all model, data, and training logic from the `/src` directory to run a reproducible training pipeline. It no longer contains any class or function definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "# This allows the notebook to find and import modules from the 'src' folder\n",
    "\n",
    "# Get the absolute path of the 'notebooks' directory (where this file lives)\n",
    "notebooks_dir = os.path.abspath(os.getcwd())\n",
    "# Get the project root (one level up)\n",
    "project_root = os.path.dirname(notebooks_dir)\n",
    "\n",
    "# Add the project root to sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# --- Import all components from our 'src' library ---\n",
    "try:\n",
    "    from src.utils.config_loader import load_config\n",
    "    from src.utils.logger import setup_logging\n",
    "    from src.models.dti_model import (\n",
    "        DTIDataset, \n",
    "        collate_fn, \n",
    "        DTIModel, \n",
    "        Trainer\n",
    "    )\n",
    "    from src.preprocessing.feature_engineer import ProteinEmbedder\n",
    "    from src.molecular_3d.conformer_generator import smiles_to_3d_graph\n",
    "    print(\"'src' modules imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Failed to import modules from 'src'.\")\n",
    "    print(f\"Make sure 'src' directory exists at: {os.path.join(project_root, 'src')}\")\n",
    "    print(f\"Details: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Load config, set up logging, and define the device. The `load_config` function now correctly resolves all paths relative to the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "# The load_config function is imported from src/utils/config_loader.py\n",
    "try:\n",
    "    config = load_config(\"config/config.yaml\") \n",
    "    print(\"Config file loaded and paths resolved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    raise\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_path=config['paths'].get('log_file', 'logs/train.log'))\n",
    "log = logging.getLogger(__name__)\n",
    "log.info(\"--- Starting New Training Run (from Notebook) ---\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(config['training']['device'] if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f\"Using device: {device}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = Path(config['paths']['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "log.info(f\"Outputs will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "Load the dataset and create training/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Loading dataset...\")\n",
    "data_path = config['paths']['dataset'] # This path is resolved by load_config\n",
    "if not Path(data_path).exists():\n",
    "    log.error(f\"Dataset file not found at: {data_path}\")\n",
    "    raise FileNotFoundError(f\"Dataset file not found at: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path, sep='\\t')\n",
    "# Using the exact same preprocessing logic from your original notebook\n",
    "df = df[['SMILES', 'sequence', 'label']].dropna().sample(frac=1, random_state=42)\n",
    "log.info(f\"Loaded {len(df)} total data points.\")\n",
    "\n",
    "# Split data\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=config['training']['val_split'], \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "log.info(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize DataLoaders\n",
    "\n",
    "We now use the `DTIDataset` class imported from `src.models.dti_model`. The heavy `ProteinEmbedder` is initialized once and passed to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Initializing Protein Embedder (ESM-2)...\")\n",
    "print(\"Initializing Protein Embedder (ESM-2)... This may take a moment.\")\n",
    "try:\n",
    "    # This heavy object is initialized once and passed to the datasets\n",
    "    protein_embedder = ProteinEmbedder(config)\n",
    "    print(\"Protein Embedder loaded.\")\n",
    "except Exception as e:\n",
    "    log.error(f\"Failed to initialize ProteinEmbedder: {e}\")\n",
    "    raise\n",
    "\n",
    "log.info(\"Creating datasets and dataloaders...\")\n",
    "\n",
    "# Create dataset instances\n",
    "# We pass the imported graph_gen_func to the dataset\n",
    "train_dataset = DTIDataset(\n",
    "    df=train_df,\n",
    "    config=config,\n",
    "    protein_embedder=protein_embedder,\n",
    "    graph_gen_func=smiles_to_3d_graph\n",
    ")\n",
    "\n",
    "val_dataset = DTIDataset(\n",
    "    df=val_df,\n",
    "    config=config,\n",
    "    protein_embedder=protein_embedder,\n",
    "    graph_gen_func=smiles_to_3d_graph\n",
    ")\n",
    "\n",
    "# Create dataloader instances\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['training'].get('num_workers', 0),\n",
    "    collate_fn=collate_fn, # Use the imported collate_fn\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['training'].get('num_workers', 0),\n",
    "    collate_fn=collate_fn, # Use the imported collate_fn\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "log.info(\"DataLoaders created successfully.\")\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model\n",
    "\n",
    "We now use the `DTIModel` class imported from `src.models.dti_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Initializing DTIModel...\")\n",
    "try:\n",
    "    model = DTIModel(config).to(device)\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log.info(f\"Model initialized with {param_count:,} trainable parameters.\")\n",
    "    print(f\"Model initialized with {param_count:,} trainable parameters.\")\n",
    "except Exception as e:\n",
    "    log.error(f\"Failed to initialize DTIModel: {e}\")\n",
    "    raise\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Training\n",
    "\n",
    "We now use the `Trainer` class imported from `src.models.dti_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Initializing Trainer...\")\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config\n",
    "    )\n",
    "    print(\"Trainer initialized.\")\n",
    "except Exception as e:\n",
    "    log.error(f\"Failed to initialize Trainer: {e}\")\n",
    "    raise\n",
    "\n",
    "log.info(\"--- Starting training loop ---\")\n",
    "print(\"--- Starting training loop ---\")\n",
    "# This will run for the number of epochs specified in config.yaml\n",
    "trainer.train() # This is the correct method name from src/models/dti_model.py\n",
    "\n",
    "log.info(\"--- Training complete ---\")\n",
    "print(\"--- Training complete ---\")\n",
    "\n",
    "print(f\"Best Validation AUROC: {trainer.best_val_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Metrics and Save\n",
    "\n",
    "Visualize the training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure plots appear inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "log.info(\"Plotting metrics...\")\n",
    "print(\"Plotting metrics...\")\n",
    "try:\n",
    "    # This is the correct method name from src/models/dti_model.py\n",
    "    fig = trainer.plot_metrics() \n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = output_dir / \"training_metrics.png\"\n",
    "    fig.savefig(plot_path)\n",
    "    log.info(f\"Training metrics plot saved to {plot_path}\")\n",
    "    print(f\"Training metrics plot saved to {plot_path}\")\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Failed to plot metrics: {e}\")\n",
    "    print(f\"Failed to plot metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " \n",
    "End of pipeline. The trained model is saved at `models/dti_model.pth` (or as specified in your config).\n",
    " \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
